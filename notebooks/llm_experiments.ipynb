{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e06d2cd-f3e5-49db-be08-7224fb13ff7f",
   "metadata": {},
   "source": [
    "# Tiny LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eb23fa-c1a5-45fd-957c-9c2a53893716",
   "metadata": {},
   "source": [
    "[![Open In Colab - ml_basics](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/proloy79/attoLLM/blob/main/notebooks/llm_experiments.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba288fec-9db1-4167-85b2-067ce835ac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally — skipping Colab setup\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Iterable, Tuple\n",
    "from pathlib import Path\n",
    "import math\n",
    "from time import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import asdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os \n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "torch.manual_seed(7)\n",
    "\n",
    "def in_colab():\n",
    "    return 'google.colab' in sys.modules\n",
    "\n",
    "if in_colab():\n",
    "    # --- Colab‑only setup ---\n",
    "    print(\"Running in Colab — setting up environment\")\n",
    "    if not os.path.exists('/content/attoLLM'):\n",
    "      !git clone https://github.com/proloy79/attoLLM.git\n",
    "    !pip install -e /content/attoLLM\n",
    "    sys.path.append('/content/attoLLM/src')\n",
    "\n",
    "else:\n",
    "    print(\"Running locally — skipping Colab setup\")\n",
    "\n",
    "from attollm.attention import scaled_dot_product_attention\n",
    "from attollm.data_loader import *\n",
    "from attollm.gpt import *\n",
    "from attollm.simple_tokenizer import SimpleTokenizer,Vocab\n",
    "from attollm.sample import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec6c242-c02f-4edc-80d0-1d3c6a89ce0f",
   "metadata": {},
   "source": [
    "## Validate single head dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace5ef33-5fc1-48af-955c-834153246744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape:  torch.Size([3, 2])\n",
      "q shape:  torch.Size([1, 3, 2])\n",
      "Single head attention tests passed!\n",
      "mask:  tensor([[[1., 0., 0.],\n",
      "         [1., 1., 0.],\n",
      "         [1., 1., 1.]]])\n",
      "scores:  tensor([[[0.7071,   -inf,   -inf],\n",
      "         [0.0000, 0.7071,   -inf],\n",
      "         [0.7071, 1.4142, 0.7071]]])\n",
      "weights:  tensor([[[1.0000, 0.0000, 0.0000],\n",
      "         [0.3302, 0.6698, 0.0000],\n",
      "         [0.2483, 0.5035, 0.2483]]])\n",
      "All missing points are filled with -inf confirming mask used properly\n",
      "All missing points have 0 weights confirming the mask was applied properly\n",
      "Causal mask test passed!\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "Q = torch.tensor([[1.,0.],\n",
    "                  [0.,1.],\n",
    "                  [1.,1.]])\n",
    "\n",
    "K = torch.tensor([[1.,0.],\n",
    "                  [1.,1.],\n",
    "                  [0.,1.]])\n",
    "\n",
    "V = torch.tensor([[1.,0.],\n",
    "                  [0.,2.],\n",
    "                  [3.,1.]])\n",
    "\n",
    "print('Q shape: ', Q.shape)\n",
    "q = Q.unsqueeze(0) # turn to batch of 1:  [1,3,2]\n",
    "k = K.unsqueeze(0)\n",
    "v = V.unsqueeze(0)\n",
    "print('q shape: ', q.shape)\n",
    "\n",
    "expected_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(2)\n",
    "expected_weights = torch.softmax(expected_scores, dim=-1)\n",
    "expected_output = torch.matmul(expected_weights, v)\n",
    "\n",
    "def test_scaled_dot_product_attention():\n",
    "    scores, weights, output = scaled_dot_product_attention(q, k, v)\n",
    "    \n",
    "    # Check shapes\n",
    "    assert scores.shape == (1, 3, 3)\n",
    "    assert weights.shape == (1, 3, 3)\n",
    "    assert output.shape == (1, 3, 2)\n",
    "\n",
    "    # Check numerical correctness\n",
    "    assert torch.allclose(scores, expected_scores, atol=1e-12), \"Scores mismatch\"\n",
    "    assert torch.allclose(weights, expected_weights, atol=1e-12), \"Weights mismatch\"\n",
    "    assert torch.allclose(output, expected_output, atol=1e-12), \"Output mismatch\"\n",
    "\n",
    "    print(\"Single head attention tests passed!\")\n",
    "\n",
    "def test_scaled_dot_product_attention_with_causal_mask(mask: Tensor):\n",
    "    scores, weights, output = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "\n",
    "    # Shape checks\n",
    "    assert scores.shape == (1, 3, 3)\n",
    "    assert weights.shape == (1, 3, 3)\n",
    "    assert output.shape == (1, 3, 2)\n",
    "\n",
    "    print('mask: ', mask)\n",
    "    print('scores: ', scores)\n",
    "    print('weights: ', weights)\n",
    "    \n",
    "    # Row 0: positions 1,2 must be -inf\n",
    "    assert scores[0,0,1] == float(\"-inf\")\n",
    "    assert scores[0,0,2] == float(\"-inf\")\n",
    "\n",
    "    # Row 1: position 2 must be -inf\n",
    "    assert scores[0,1,2] == float(\"-inf\")\n",
    "\n",
    "    # Row 2: no masking\n",
    "    assert not torch.isinf(scores[0,2]).any()\n",
    "    print('All missing points are filled with -inf confirming mask used properly')\n",
    "    \n",
    "    # Check that softmax respects the mask:\n",
    "    # Row 0 must put all probability on position 0\n",
    "    assert torch.allclose(weights[0,0], torch.tensor([1.,0.,0.]), atol=1e-12)\n",
    "\n",
    "    # Row 1 must distribute only over positions 0 and 1\n",
    "    assert weights[0,1,2] == 0.\n",
    "\n",
    "    # Row 2 unchanged from unmasked case\n",
    "    assert torch.allclose(weights[0,2], expected_weights[0,2], atol=1e-12)\n",
    "    print('All missing points have 0 weights confirming the mask was applied properly')\n",
    "    print(\"Causal mask test passed!\")\n",
    "\n",
    "test_scaled_dot_product_attention()\n",
    "\n",
    "mask = torch.tril(torch.ones(3, 3)).unsqueeze(0)  # shape (1,3,3)\n",
    "test_scaled_dot_product_attention_with_causal_mask(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ebd5a1-8f23-45c1-a98b-3519f4a73761",
   "metadata": {},
   "source": [
    "## Tiny GPT that predicts what will come next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6c9ebb-1934-4c3c-9946-4ad829053ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:  4 vocab size:  45 dl len 550\n",
      "Config: {'vocab_size': 45, 'block_size': 128, 'd_model': 128, 'n_head': 4, 'n_layer': 2, 'd_ff': 512, 'dropout': 0.1, 'pos_type': 'sinusoidal', 'tie_weights': True}\n",
      "Dataset tokens: 2330\n",
      "running epoch 0\n",
      "step     0 lr 0.00007 loss 3.7943\n",
      "step    50 lr 0.00030 loss 3.1065\n",
      "step   100 lr 0.00030 loss 3.0084\n",
      "step   150 lr 0.00030 loss 3.0224\n",
      "step   200 lr 0.00030 loss 3.0588\n",
      "step   250 lr 0.00030 loss 3.0509\n",
      "running epoch 1\n",
      "step     0 lr 0.00030 loss 3.0072\n",
      "step    50 lr 0.00030 loss 2.9962\n",
      "step   100 lr 0.00030 loss 2.9896\n",
      "step   150 lr 0.00030 loss 3.0418\n",
      "step   200 lr 0.00030 loss 3.0809\n",
      "step   250 lr 0.00030 loss 2.9815\n",
      "Done. steps=300 time=23.6s\n",
      "Checkpoints saved to : ../data/processed/gpt_outputs.pt\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "level=\"char\" #\"word\"\n",
    "warmup_steps=8\n",
    "epochs=2\n",
    "steps=300\n",
    "block_size=128\n",
    "batch_size=4\n",
    "lr=3e-4\n",
    "d_model = 128\n",
    "n_head = 4\n",
    "n_layer = 2\n",
    "d_ff = 512\n",
    "dropout = 0.1\n",
    "pos_type = \"sinusoidal\" #\"learned\"\n",
    "out_file_path = '/content/attoLLM/data/processed/gpt_outputs.pt' if in_colab() else './../data/processed/gpt_outputs.pt'\n",
    "#############################\n",
    "\n",
    "\n",
    "text = load_texts(['/content/attoLLM/data/raw/sample.txt' if in_colab() else './../data/raw/sample.txt'])\n",
    "ids_info = build_ids_with_tokenizer(text, level)\n",
    "ds = LMSequenceDataset(ids_info.ids, block_size) # Slice stream into windows\n",
    "dl = DataLoader(ds, batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "vocab_size=ids_info.vocab_size\n",
    "print('batch size: ', batch_size, 'vocab size: ', vocab_size, 'dl len', len(dl))\n",
    "\n",
    "cfg = GPTConfig(vocab_size=vocab_size, block_size=block_size, d_model=d_model, n_head=n_head, n_layer=n_layer, d_ff=d_ff, dropout=dropout, pos_type=pos_type)\n",
    "model =GPT(cfg)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = None\n",
    "if warmup_steps > 0:\n",
    "    # Linear warmup from 0 -> 1 over warmup_steps\n",
    "    def lr_lambda(step: int) -> float:\n",
    "        return min(1.0, (step + 1) / float(warmup_steps))\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
    "\n",
    "print(\"Config:\", asdict(cfg))\n",
    "print(\"Dataset tokens:\", ds.ids.numel())\n",
    "\n",
    "t0 = time()\n",
    "model.train()\n",
    "\n",
    "for epoch in range(max(1, epochs)):\n",
    "    step = 0\n",
    "    print(f'running epoch {epoch}')\n",
    "    \n",
    "    for x, y in dl:\n",
    "        #print(f'running step {step}')\n",
    "        if steps and step >= steps:\n",
    "            break\n",
    "                \n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits, loss = model(x, targets=y, pad_id=ids_info.pad_id)\n",
    "        \n",
    "        assert loss is not None\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        if step % 50 == 0:\n",
    "            lr_now = opt.param_groups[0][\"lr\"]\n",
    "            print(\n",
    "                f\"step {step:5d} lr {lr_now:.5f} \"\n",
    "                f\"loss {loss.detach().item():.4f}\"\n",
    "            )\n",
    "        step += 1\n",
    "            \n",
    "dt = time() - t0\n",
    "print(f\"Done. steps={step} time={dt:.1f}s\")\n",
    "\n",
    "# Save checkpoint\n",
    "out = Path(out_file_path)\n",
    "out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ckpt = {\n",
    "    \"config\": asdict(cfg),\n",
    "    \"model_state\": model.state_dict(),\n",
    "}\n",
    "\n",
    "#print(ids_info.token_to_id)\n",
    "\n",
    "# Save tokenizer metadata if available for easier sampling later\n",
    "if ids_info.id_to_token is not None:\n",
    "    ckpt[\"tokenizer\"] = {\n",
    "        \"level\": ids_info.level,\n",
    "        \"id_to_token\": ids_info.id_to_token,\n",
    "        \"token_to_id\": ids_info.token_to_id,\n",
    "        \"pad_id\": ids_info.pad_id,\n",
    "        \"unk_id\": ids_info.unk_id,\n",
    "    }\n",
    "torch.save(ckpt, out)\n",
    "print(\"Checkpoints saved to :\", out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce448cf5-4070-41e9-acb0-6d14aac230cf",
   "metadata": {},
   "source": [
    "## Load the checkpoint and test a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b0e72e-435f-4359-9a00-87f1b50df7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated text:  Philosophy isa polnahcsoeeot lec erseaotitstc lreo ppeplr til sfieaecci sioitroyas  anhctethnsnan neyou iieuyr  aneipfeeydia, i ihnct\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "load checkpoint\n",
    "rebuild model from config\n",
    "load weights\n",
    "restore tokenizer\n",
    "encode prompt\n",
    "run sampling loop\n",
    "decode output\n",
    "\"\"\"\n",
    "\n",
    "# rebuild model from saved config and set to eval state\n",
    "ckpt = torch.load(out_file_path)\n",
    "cfg = GPTConfig(**ckpt[\"config\"])\n",
    "\n",
    "model = GPT(cfg)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "# restore tokenizer\n",
    "tok_info = ckpt[\"tokenizer\"]\n",
    "reloaded_id_to_token = tok_info[\"id_to_token\"]\n",
    "reloaded_token_to_id = tok_info[\"token_to_id\"]\n",
    "reloaded_pad_id = tok_info[\"pad_id\"]\n",
    "reloaded_unk_id = tok_info[\"unk_id\"]\n",
    "\n",
    "#print(reloaded_token_to_id)\n",
    "\n",
    "reloaded_tok=SimpleTokenizer(Vocab(reloaded_token_to_id,\n",
    "    reloaded_id_to_token,\n",
    "    reloaded_pad_id,\n",
    "    reloaded_unk_id))\n",
    "\n",
    "prompt = \"Philosophy is\"\n",
    "test_token_ids = torch.tensor(reloaded_tok.encode(prompt), dtype=torch.long).unsqueeze(0)\n",
    "#print(test_token_ids)\n",
    "gen = sample(model, test_token_ids, max_new_tokens=120, temperature=0.9, top_p=0.95)\n",
    "generated_text = reloaded_tok.decode(gen.tolist()[0])\n",
    "print('generated text: ', generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b64d056-da7d-4d6b-bfb9-55efe316e35d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
