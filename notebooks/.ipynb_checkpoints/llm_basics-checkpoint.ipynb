{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dece7f36-079e-4876-904e-6ee37375c819",
   "metadata": {},
   "source": [
    "# LLM Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2324f28-7224-464a-82c9-26816ad49812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf601289-5ac6-433a-8b4d-ba4b10774a40",
   "metadata": {},
   "source": [
    "## 1) Play with the simple tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d796528f-b8ce-4a7f-bd59-5ba621fcdff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 3, 2, 2, 4, 5, 10, 4, 6, 2, 8, 5, 1, 1, 8, 8, 1, 9]\n",
      "vocab items count(V):  17\n",
      "decode will drop chars b,u,y as they are not in mini.txt and only see the chars that are in the vocab\n",
      "Hello world dd.\n"
     ]
    }
   ],
   "source": [
    "from attollm.simple_tokenizer import SimpleTokenizer\n",
    "\"\"\"\n",
    "Create a vocab from mini.txt using char tokenization.\n",
    "Then call the tokenizer to encode the provided text\n",
    "\"\"\"\n",
    "text = Path('../data/raw/mini.txt').read_text(encoding=\"utf-8\")\n",
    "tok = SimpleTokenizer.from_text(text, level='char')\n",
    "ids = tok.encode('Hello world buddy.') # Text -> ids\n",
    "print(ids)\n",
    "V=len(tok.vocab)\n",
    "print('vocab items count(V): ', V)\n",
    "print('decode will drop chars b,u,y as they are not in mini.txt and only see the chars that are in the vocab')\n",
    "text=tok.decode(ids)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e786764c-48d1-4ea7-9dea-31b5f6eb9446",
   "metadata": {},
   "source": [
    "## 2) Combined mask - right padding + causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2097d8-43e9-4fde-9082-3a48a61259f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attollm.embedding_block import EmbeddingBlock\n",
    "\n",
    "batch = [tok.encode('Hello'), tok.encode('vectors')]\n",
    "combined_mask = EmbeddingBlock.create_combined_mask(tok.pad, batch)\n",
    "print('The combined mask(right padding + causal) sets a default value(0 in the example below) to empty points, so that they do not interfere in the actual weight calculation')\n",
    "print(combined_mask)\n",
    "print(combined_mask.shape,\"\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "for b in range(2):\n",
    "    sns.heatmap(\n",
    "        combined_mask[b].detach().cpu(),\n",
    "        cmap=\"Blues\",\n",
    "        square=True,\n",
    "        cbar=True,\n",
    "        ax=axes[b],\n",
    "        linewidths=0.5,        # thickness of grid lines\n",
    "        linecolor=\"white\"\n",
    "    )\n",
    "    axes[b].set_title(f\"Batch {b}\", fontsize=10)\n",
    "    axes[b].set_xlabel(\"Key positions\", fontsize=10)\n",
    "    axes[b].set_ylabel(\"Query positions\", fontsize=10)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.suptitle(\"Combined Mask for Both Batches\", fontsize=11, y=1.05)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe blank (light) cells correspond to padded positions. These positions are masked out, so the model assigns them zero attention weight.\")\n",
    "print(\"\\nIn a combined mask, each row represents how far a token can attend into the past. Lower rows correspond to later tokens, so they have more allowed attention positions, which appear as more colored cells. Padded rows remain empty because they are fully masked out.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd020c-25fe-4073-8684-9cc1743d85d4",
   "metadata": {},
   "source": [
    "## 3) Prepare feed for MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396900f6-eb66-4949-9275-48ccfe1d04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attollm.attention import scaled_dot_product_attention\n",
    "\n",
    "lines = text.splitlines()\n",
    "\n",
    "batch_ids = [tok.encode(line) for line in lines]\n",
    "max_len = max(len(seq) for seq in batch_ids)\n",
    "mask = EmbeddingBlock.create_combined_mask(tok.pad, batch_ids)\n",
    "ids = EmbeddingBlock.pad_batch_ids(tok.pad, batch_ids)\n",
    "\n",
    "d_model = 16\n",
    "print('No of params(d_model): ', d_model)\n",
    "embed = EmbeddingBlock(len(tok.vocab), d_model, max_len)\n",
    "x = embed(ids)   # (B, T, D)\n",
    "\n",
    "# Attention\n",
    "q = x; k = x; v = x\n",
    "scores, weights, y = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "B=len(lines)\n",
    "print('No of batches(B): ', B)\n",
    "T=mask.size(1)\n",
    "print('No of timesteps(T): ', T)\n",
    "\n",
    "print('mha input shape[B,T,d_model]: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca0aa9a-af59-43f2-90b9-a6b4f9671c8a",
   "metadata": {},
   "source": [
    "## 4) Play with MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd40dbf-d541-4b2b-9a21-a1eee35a78bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attollm.mha import MultiHeadAttention\n",
    "\n",
    "num_heads=4\n",
    "\n",
    "print('dropout helps regularize the output by randomly dropping a % of attentions')\n",
    "dropout=0.1\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads, .1)\n",
    "print('mha takes the input, projects it into Q, K, and V, splits each into multiple heads of size d_model/num_heads, performs scaled dotâ€‘product attention in each head, then combines all heads back to the original shape')\n",
    "mha_out=mha(y, mask)\n",
    "\n",
    "print('mha output is same shape as single linear feed : [B,T,d_model]')\n",
    "print('mha output shape: ', mha_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c7fd15-59a1-4344-8dfc-c4449f4c300c",
   "metadata": {},
   "source": [
    "## 5) Tranformer Block(MHA->Norm->FF->Norm->LM Head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81525dbc-40cd-47ec-846c-0c90c32388c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attollm.transformer_block import TransformerBlock\n",
    "\n",
    "d_ff=2*d_model\n",
    "transform = TransformerBlock(d_model, num_heads, d_ff, V, .1,)\n",
    "logits=transform(y)\n",
    "print('transform block output will be projected as [B,T,V]')\n",
    "print('logits shape: ', logits.shape)\n",
    "\n",
    "print('next step will be to pass targets so that loss can be computed and gradient descent applied to train this llm to predict what comes next')\n",
    "print('also try Weight Tying ie setting the embedded weights to the final LM head weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7122566e-f0e1-4334-81c1-ab80aa02c422",
   "metadata": {},
   "source": [
    "## 6) Visualise how backpropagation (loss.backward) reduces loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae2aa1-17ba-49f8-8350-dcc07115cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=torch.tensor([3,1.2])\n",
    "b1=torch.tensor(0.5)\n",
    "N=500\n",
    "X1=torch.randn(N, 2)\n",
    "y1=X1@w1 + b1+ 0.1 * torch.randn(N)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = torch.nn.Linear(2, 1)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "model.train()\n",
    "\n",
    "print('--------------')\n",
    "print(f\"{'step':<6}{'loss'}\")\n",
    "print('--------------')\n",
    "for step in range(401):  # Simple training loop\n",
    "    opt.zero_grad()  # Clear stale gradients\n",
    "    pred = model(X1).squeeze(-1)  # Forward pass over dataset\n",
    "    loss = loss_fn(pred, y1)  # Compute scalar loss\n",
    "    loss.backward()  # Backprop into parameters\n",
    "    opt.step()  # Apply gradient update\n",
    "    if step % 50 == 0:\n",
    "        print(f\"{step:<6}{loss.item():.4f}\")\n",
    "\n",
    "print('loss reduces with propagation and coverges to min after ~350 steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ccfb4e-07e1-460d-a8f1-8b3003f8a6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bdd4ae-1fca-42a5-9e38-a50ea7ec489f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
